{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "\n",
    "seed=122334445\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Mcg   Gvh   Alm   Mit  Erl  Pox   Vac   Nuc     class\n",
      "0  0.51  0.40  0.56  0.17  0.5  0.5  0.49  0.22  negative\n",
      "1  0.40  0.39  0.60  0.15  0.5  0.0  0.58  0.30  negative\n",
      "2  0.40  0.42  0.57  0.35  0.5  0.0  0.53  0.25  negative\n",
      "3  0.46  0.44  0.52  0.11  0.5  0.0  0.50  0.22  negative\n",
      "4  0.47  0.39  0.50  0.11  0.5  0.0  0.49  0.40  negative\n",
      "Dataset X shape: (514, 8)\n",
      "Dataset Y shape: (514,)\n",
      "x_train shape: (462, 8)\n",
      "y_train shape: (462,)\n",
      "x_test shape: (52, 8)\n",
      "y_test shape: (52,)\n"
     ]
    }
   ],
   "source": [
    "header = ['Mcg', 'Gvh', 'Alm', 'Mit', 'Erl', 'Pox', 'Vac', 'Nuc', 'class']\n",
    "df = pd.read_csv('data/yeast-2_vs_4.dat', names=header, skiprows=13)\n",
    "print(df.head())\n",
    "df['class'] = df['class'].apply(lambda x:0 if x=='negative' else 1)\n",
    "\n",
    "df_np = df.to_numpy()\n",
    "x = df_np[:,:-1]\n",
    "y = df_np[:,-1]\n",
    "\n",
    "x = (x - x.mean(axis=0)) / x.var(axis=0)\n",
    "\n",
    "kfold = KFold(n_splits=5, random_state=seed, shuffle=True)\n",
    "\n",
    "ensemble_sizes = [10, 50, 100]\n",
    "\n",
    "print('Dataset X shape:', x.shape)\n",
    "print('Dataset Y shape:', y.shape)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=seed)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoostM2:\n",
    "    def __init__(self, x, y, n_classifier, base=None, weights=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize AdaBoost M2 (Weight init is same as M1)\n",
    "        \n",
    "        :param x: input feauture in shape of (samples, features)\n",
    "        :param y: input label in shape of (samples, )\n",
    "        :param base: base classifier (default Decision Tree)\n",
    "        :param n_classifier: number of base classifier in ensemble\n",
    "        :param weights: init model with pretrained weights\n",
    "        \n",
    "        :return: A AdaBoost model\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.base = base\n",
    "        if self.base is None:\n",
    "            self.base = DecisionTreeClassifier\n",
    "        self.n_classifier = n_classifier\n",
    "        self.classifiers = []\n",
    "        self.weights = weights\n",
    "        self.alpha = []\n",
    "        self.bad_classifier_idx = []\n",
    "        \n",
    "        # init ensemble\n",
    "        for n in range(self.n_classifier):\n",
    "            self.classifiers.append(self.base(**kwargs))\n",
    "        \n",
    "        if self.weights is None:\n",
    "            # init weights using uniform distrobution\n",
    "            self.weights = np.ones(len(self.x)) / len(self.x)\n",
    "            \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict the class of given instance\n",
    "        \n",
    "        :param x: input feauture in shape of (samples, features)\n",
    "        \n",
    "        :return: a prediction of classes in label encoded form with shape of (samples, )\n",
    "        \"\"\"\n",
    "        \n",
    "        prediction = np.zeros((len(x),))\n",
    "        for idx in range(len(x)):\n",
    "            prediction[idx] = self.__predict_single_instance(x[idx].reshape(1, -1))\n",
    "        return prediction\n",
    "            \n",
    "    def __predict_single_instance(self, x):\n",
    "        \"\"\"\n",
    "        Predict the class of given instance\n",
    "        \n",
    "        :param x: input feauture in shape of (1, features)\n",
    "        \n",
    "        :return: a prediction of classes in label encoded form with shape of (1, )\n",
    "        \"\"\"\n",
    "        p = np.zeros((1, 2))\n",
    "        for n in range(self.n_classifier):\n",
    "            if n not in self.bad_classifier_idx:\n",
    "                if self.classifiers[n].predict(x) == 1:\n",
    "                    p[0,1] += np.log(1 / (self.alpha[n]+1e-10))\n",
    "                else:\n",
    "                    p[0,0] += np.log(1 / (self.alpha[n]+1e-10))\n",
    "        p[:,1] += 1e-10 \n",
    "        return np.argmax(p, axis=1)\n",
    "    \n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Train the ensemble using base weak classifiers\n",
    "        \"\"\"\n",
    "        for t in range(self.n_classifier):            \n",
    "            \n",
    "            # training weak classifier\n",
    "            self.classifiers[t].fit(self.x, self.y, sample_weight=self.weights)\n",
    "                        \n",
    "            # calculating loss = sum of missclassified weights\n",
    "            miss_w = self.weights[(self.classifiers[t].predict(self.x) != self.y).nonzero()[0]]\n",
    "            loss = np.sum(miss_w) / 2 \n",
    "            \n",
    "            # calculating beta\n",
    "            a = loss / (1 - loss)\n",
    "            self.alpha.append(a)\n",
    "            \n",
    "            # drop classifiers with acc < 0.5\n",
    "            if self.classifiers[t].score(self.x, self.y) <= 0.5:\n",
    "                self.bad_classifier_idx.append(t)\n",
    "                continue\n",
    "#             print(a, self.n_classifier, self.classifiers[t].score(self.x, self.y))\n",
    "            # update weights\n",
    "            correct_pred_idx = (self.classifiers[t].predict(self.x) == self.y).nonzero()[0]\n",
    "            self.weights[correct_pred_idx] = self.weights[correct_pred_idx] * a\n",
    "            \n",
    "            # normalize weights\n",
    "            z = np.sum(self.weights)\n",
    "            self.weights = np.array([w / z for w in self.weights])\n",
    "             \n",
    "    \n",
    "    def score(self, x, y):\n",
    "        p = self.predict(x)\n",
    "        return np.sum((p == y)*1) / len(y)   \n",
    "\n",
    "# Test\n",
    "# model = AdaBoostM2(x=x_train, y=y_train, n_classifier=100, base=DecisionTreeClassifier, max_depth=1)\n",
    "# model.fit()\n",
    "# model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of ensemble with size of #10:\n",
      " [0.91262135922330101, 0.94174757281553401, 0.970873786407767, 0.91262135922330101, 1.0] ---> AVG=0.9475728155339805\n",
      "Accuracy of ensemble with size of #50:\n",
      " [0.91262135922330101, 0.94174757281553401, 0.970873786407767, 0.91262135922330101, 1.0] ---> AVG=0.9475728155339805\n",
      "Accuracy of ensemble with size of #100:\n",
      " [0.91262135922330101, 0.94174757281553401, 0.970873786407767, 0.91262135922330101, 1.0] ---> AVG=0.9475728155339805\n"
     ]
    }
   ],
   "source": [
    "adaboost_accuracies = []  # accuracies of different ensembles given 5 folds\n",
    "for es in ensemble_sizes:\n",
    "    kf_acc = []  # accuracies of 5 fold\n",
    "    for train_index, test_index in kfold.split(x):\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model = AdaBoostM2(x=x_train, y=y_train, n_classifier=es, base=DecisionTreeClassifier, max_depth=1)\n",
    "        model.fit()\n",
    "        kf_acc.append(model.score(x_test, y_test))\n",
    "    adaboost_accuracies.append(kf_acc)\n",
    "for idx,f in enumerate(adaboost_accuracies):\n",
    "    print('Accuracy of ensemble with size of #{}:\\n {} ---> AVG={}'.format(\n",
    "        ensemble_sizes[idx], f, np.mean(adaboost_accuracies[idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RUSBoost:\n",
    "    def __init__(self, x, y, n_classifier, base=None, weights=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize RUSBoost\n",
    "        \n",
    "        :param x: input feauture in shape of (samples, features)\n",
    "        :param y: input label in shape of (samples, )\n",
    "        :param base: base classifier (default Decision Tree)\n",
    "        :param n_classifier: number of base classifier in ensemble\n",
    "        :param weights: init model with pretrained weights\n",
    "        \n",
    "        :return: A RUSBoost model\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.base = base\n",
    "        if self.base is None:\n",
    "            self.base = DecisionTreeClassifier\n",
    "        self.n_classifier = n_classifier\n",
    "        self.classifiers = []\n",
    "        self.weights = weights\n",
    "        self.alpha = []\n",
    "        self.bad_classifier_idx = []\n",
    "        \n",
    "        # init ensemble\n",
    "        for n in range(self.n_classifier):\n",
    "            self.classifiers.append(self.base(**kwargs))\n",
    "        \n",
    "        if self.weights is None:\n",
    "            # init weights using uniform distrobution\n",
    "            self.weights = np.ones((len(self.x))) / len(self.x)\n",
    "            \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict the class of given instance\n",
    "        \n",
    "        :param x: input feauture in shape of (samples, features)\n",
    "        \n",
    "        :return: a prediction of classes in label encoded form with shape of (samples, )\n",
    "        \"\"\"\n",
    "        \n",
    "        prediction = np.zeros((len(x),))\n",
    "        for idx in range(len(x)):\n",
    "            prediction[idx] = self.__predict_single_instance(x[idx].reshape(1, -1))\n",
    "        return prediction\n",
    "            \n",
    "    def __predict_single_instance(self, x):\n",
    "        \"\"\"\n",
    "        Predict the class of given instance\n",
    "        \n",
    "        :param x: input feauture in shape of (1, features)\n",
    "        \n",
    "        :return: a prediction of classes in label encoded form with shape of (1, )\n",
    "        \"\"\"\n",
    "        p = np.zeros((1, 2))\n",
    "        for n in range(self.n_classifier):\n",
    "            if n not in self.bad_classifier_idx:\n",
    "                if self.classifiers[n].predict(x) == 1:\n",
    "                    p[0,1] += np.log(1 / (self.alpha[n]+1e-10))\n",
    "                else:\n",
    "                    p[0,0] += np.log(1 / (self.alpha[n]+1e-10))\n",
    "        p[:,1] += 1e-10\n",
    "        return np.argmax(p, axis=1)\n",
    "    \n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Train the ensemble using RUS data boosting and base weak classifiers\n",
    "        \"\"\"\n",
    "        for t in range(self.n_classifier):            \n",
    "            # random under sampling\n",
    "            rus_idx = self.__undersample()\n",
    "\n",
    "            # training weak classifier\n",
    "            self.classifiers[t].fit(self.x[rus_idx], self.y[rus_idx], self.weights[rus_idx])\n",
    "            \n",
    "            # calculating loss = sum of missclassified weights            \n",
    "            miss_w = self.weights[(self.classifiers[t].predict(self.x) != self.y).nonzero()[0]]\n",
    "            loss = np.sum(miss_w) / 2 \n",
    "            \n",
    "            # calculating beta\n",
    "            a = loss / (1 - loss)\n",
    "            self.alpha.append(a)\n",
    "            \n",
    "            # drop bad classifiers\n",
    "            if self.classifiers[t].score(self.x, self.y) <= 0.5:\n",
    "                self.bad_classifier_idx.append(t)\n",
    "                continue\n",
    "            \n",
    "            # update weights\n",
    "            correct_pred_idx = (self.classifiers[t].predict(self.x) == self.y).nonzero()[0]\n",
    "            self.weights[correct_pred_idx] = self.weights[correct_pred_idx] * a\n",
    "            \n",
    "            # normalize weights\n",
    "            z = np.sum(self.weights)\n",
    "            self.weights = np.array([w / z for w in self.weights])\n",
    "             \n",
    "    \n",
    "    def score(self, x, y):\n",
    "        p = self.predict(x)\n",
    "        return (p == y).nonzero()[0].__len__() / len(y)\n",
    "           \n",
    "    def __undersample(self):\n",
    "        \"\"\"\n",
    "        Generates a random unique subset of majority data as same size as minority and return the indices\n",
    "        \n",
    "        :return: A sorted list of indices with shape of (2*minority_data, )\n",
    "        \"\"\"\n",
    "        pos_size = len((self.y==1).nonzero()[0])\n",
    "        neg_size = len((self.y==0).nonzero()[0])\n",
    "        pos_data = self.x[self.y==1]\n",
    "        neg_data = self.x[self.y==0]\n",
    "        \n",
    "        if pos_size > neg_size:\n",
    "            self.major_data = pos_data\n",
    "            self.minor_data = neg_data\n",
    "            self.minor = 0\n",
    "        else:\n",
    "            self.minor_data = pos_data\n",
    "            self.major_data = neg_data\n",
    "            self.minor = 1\n",
    "        # getting index of sampled intances for enabling correct weight update\n",
    "        minor_idx = (self.y == self.minor).nonzero()[0]\n",
    "        major_idx = (self.y == int(not self.minor)).nonzero()[0]\n",
    "        major_idx = np.array(random.sample(list(major_idx), len(self.minor_data)))\n",
    "        return sorted(np.concatenate((minor_idx, major_idx)))\n",
    "    \n",
    "# test\n",
    "# model = RUSBoost(x=x_train, y=y_train, n_classifier=30, base=DecisionTreeClassifier, max_depth=1)\n",
    "# model.fit()\n",
    "# model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of ensemble with size of #10:\n",
      " [0.9320388349514563, 0.9029126213592233, 0.970873786407767, 0.912621359223301, 0.9509803921568627] ---> AVG=0.933885398819722\n",
      "Accuracy of ensemble with size of #50:\n",
      " [0.9320388349514563, 0.9514563106796117, 0.970873786407767, 0.941747572815534, 0.9705882352941176] ---> AVG=0.9533409480296973\n",
      "Accuracy of ensemble with size of #100:\n",
      " [0.9029126213592233, 0.941747572815534, 0.9902912621359223, 0.8737864077669902, 0.9705882352941176] ---> AVG=0.9358652198743576\n"
     ]
    }
   ],
   "source": [
    "rusboost_accuracies = []  # accuracies of different ensembles given 5 folds\n",
    "for es in ensemble_sizes:\n",
    "    kf_acc = []  # accuracies of 5 fold\n",
    "    for train_index, test_index in kfold.split(x):\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model = RUSBoost(x=x_train, y=y_train, n_classifier=es, base=DecisionTreeClassifier, max_depth=1)\n",
    "        model.fit()\n",
    "        kf_acc.append(model.score(x_test, y_test))\n",
    "    rusboost_accuracies.append(kf_acc)\n",
    "for idx,f in enumerate(rusboost_accuracies):\n",
    "    print('Accuracy of ensemble with size of #{}:\\n {} ---> AVG={}'.format(\n",
    "        ensemble_sizes[idx], f, np.mean(rusboost_accuracies[idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMOTEBoost:\n",
    "    def __init__(self, x, y, n_classifier, k=5, smote_ratio=100, base=None, weights=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize AdaBoost M2 (Weight init is same as M1)\n",
    "        \n",
    "        :param x: input feauture in shape of (samples, features)\n",
    "        :param y: input label in shape of (samples, )\n",
    "        :param base: base classifier (default Decision Tree)\n",
    "        :param n_classifier: number of base classifier in ensemble\n",
    "        :param weights: init model with pretrained weights\n",
    "        :param smote_ratio: the ratio of smoteing data\n",
    "        :param k: number of nearest neighbors in SMOTE\n",
    "        \n",
    "        :return: A SMOTEBoost model\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.base = base\n",
    "        if self.base is None:\n",
    "            self.base = DecisionTreeClassifier\n",
    "        self.n_classifier = n_classifier\n",
    "        self.smote_ratio = smote_ratio  # alias N\n",
    "        self.k = k\n",
    "        self.classifiers = []\n",
    "        self.weights = weights\n",
    "        self.alpha = []\n",
    "        self.newindex = 0  # to count SMOTEed samples\n",
    "        self.synthetic = []  # SMOTEed samples\n",
    "        self.bad_classifier_idx = []\n",
    "        \n",
    "        # init ensemble\n",
    "        for n in range(self.n_classifier):\n",
    "            self.classifiers.append(self.base(**kwargs))\n",
    "            \n",
    "    def __SMOTE(self):\n",
    "        \"\"\"\n",
    "        Applies SMOTE on data\n",
    "        \n",
    "        :return: SMOTEed data in shape of (N*T/100)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.synthetic = []  # reinit synthetic for new SMOTEing\n",
    "        \n",
    "        pos_size = len((self.y==1).nonzero()[0])\n",
    "        neg_size = len((self.y==0).nonzero()[0])\n",
    "        pos_data = self.x[self.y==1]\n",
    "        neg_data = self.x[self.y==0]\n",
    "        \n",
    "        if pos_size > neg_size:\n",
    "            self.major_data = pos_data\n",
    "            self.minor_data = neg_data\n",
    "            self.minor = 0\n",
    "        else:\n",
    "            self.minor_data = pos_data\n",
    "            self.major_data = neg_data\n",
    "            self.minor = 1\n",
    "        \n",
    "        N = self.smote_ratio\n",
    "        T = len(self.minor_data)\n",
    "        T = int(N * T / 100)\n",
    "             \n",
    "        while T != 0:\n",
    "            i = np.random.randint(1, len(self.minor_data)) - 1\n",
    "            self.__populate(i, self.__KNN(i))\n",
    "            T = T - 1\n",
    "        \n",
    "        return np.array(self.synthetic)\n",
    "        \n",
    "    def __KNN(self, idx):\n",
    "        \"\"\"\n",
    "        Applies SMOTE on data\n",
    "        \n",
    "        :param idx: index of an instance of input data (x)\n",
    "        :return: k indices of nearest neighbor to the given instance\n",
    "        \"\"\"\n",
    "        \n",
    "        distances = []\n",
    "        for i in range(len(self.minor_data)):\n",
    "            if i != idx:\n",
    "                distances.append(((np.sqrt(np.sum(self.minor_data[idx] - self.minor_data[i])**2)), i))\n",
    "        # get k nearest\n",
    "        distances = sorted(distances, key=lambda x:x[0])\n",
    "        return [en[1] for en in distances[:self.k]]\n",
    "                \n",
    "    def __populate(self, i, knn):\n",
    "        \"\"\"\n",
    "        Create synthetic instances given particular instance and its K nearest neighbors\n",
    "        \n",
    "        :param i: index of current sample to generated SMOTE from\n",
    "        :param knn: index of k nearest neighbors of current sample i\n",
    "        :return: None - Updates self.synthetic \n",
    "        \"\"\"\n",
    "        \n",
    "        nn = np.random.randint(0, len(knn))\n",
    "        diff = self.minor_data[knn[nn]] - self.minor_data[i]\n",
    "        gap = np.random.randn(self.minor_data.shape[1])\n",
    "        self.synthetic.insert(self.newindex, self.minor_data[i] + gap * diff)\n",
    "        \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Train the ensemble using base weak classifiers\n",
    "        \"\"\"\n",
    "        syn_data = self.__SMOTE()  # just to determine sizes\n",
    "        if self.weights is None:\n",
    "            # init weights using uniform distrobution\n",
    "            self.weights = np.ones((len(syn_data)+len(self.x))) / (len(syn_data)+len(self.x))\n",
    "        \n",
    "        for t in range(self.n_classifier):            \n",
    "            # SMOTE data\n",
    "            syn_data = self.__SMOTE()\n",
    "            x_smote = np.concatenate((self.x, syn_data))\n",
    "            y_smote = np.concatenate((self.y, np.ones((len(syn_data)))*self.minor))\n",
    "            \n",
    "            # training weak classifier\n",
    "            self.classifiers[t].fit(x_smote, y_smote, self.weights)\n",
    "            \n",
    "            # calculating loss = sum of missclassified weights\n",
    "            miss_w = self.weights[(self.classifiers[t].predict(x_smote) != y_smote).nonzero()[0]]\n",
    "            loss = np.sum(miss_w) / 2 \n",
    "            \n",
    "            # calculating beta\n",
    "            a = loss / (1 - loss)\n",
    "            self.alpha.append(a)\n",
    "            \n",
    "            # drop bad classifiers\n",
    "            if self.classifiers[t].score(self.x, self.y) <= 0.5:\n",
    "                self.bad_classifier_idx.append(t)\n",
    "                continue\n",
    "            \n",
    "            # update weights\n",
    "            correct_pred_idx = (self.classifiers[t].predict(x_smote) == y_smote).nonzero()[0]\n",
    "            self.weights[correct_pred_idx] = self.weights[correct_pred_idx] * a\n",
    "            \n",
    "            # normalize weights\n",
    "            z = np.sum(self.weights)\n",
    "            self.weights = np.array([w / z for w in self.weights])\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict the class of given instance\n",
    "        \n",
    "        :param x: input feauture in shape of (samples, features)\n",
    "        \n",
    "        :return: a prediction of classes in label encoded form with shape of (samples, )\n",
    "        \"\"\"\n",
    "        \n",
    "        prediction = np.zeros((len(x),))\n",
    "        for idx in range(len(x)):\n",
    "            prediction[idx] = self.__predict_single_instance(x[idx].reshape(1, -1))\n",
    "        return prediction\n",
    "            \n",
    "    def __predict_single_instance(self, x):\n",
    "        \"\"\"\n",
    "        Predict the class of given instance\n",
    "        \n",
    "        :param x: input feauture in shape of (1, features)\n",
    "        \n",
    "        :return: a prediction of classes in label encoded form with shape of (1, )\n",
    "        \"\"\"\n",
    "        p = np.zeros((1, 2))\n",
    "        for n in range(self.n_classifier):\n",
    "            if n not in self.bad_classifier_idx:\n",
    "                if self.classifiers[n].predict(x) == 1:\n",
    "                    p[0,1] += np.log(1 / (self.alpha[n]+1e-10))\n",
    "                else:\n",
    "                    p[0,0] += np.log(1 / (self.alpha[n]+1e-10))\n",
    "        p[:,1] += 1e-10\n",
    "        return np.argmax(p, axis=1)\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        p = self.predict(x)\n",
    "        return (p == y).nonzero()[0].__len__() / len(y)\n",
    "        \n",
    "# test\n",
    "# model = SMOTEBoost(x=x_train, y=y_train, n_classifier=30, smote_ratio=200, base=DecisionTreeClassifier, max_depth=1)\n",
    "# model.fit()\n",
    "# model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of ensemble with size of #10:\n",
      " [0.8932038834951457, 0.941747572815534, 0.9902912621359223, 0.9320388349514563, 0.9215686274509803] ---> AVG=0.9357700361698077\n",
      "Accuracy of ensemble with size of #50:\n",
      " [0.8932038834951457, 0.9514563106796117, 0.9805825242718447, 0.941747572815534, 0.9803921568627451] ---> AVG=0.9494764896249762\n",
      "Accuracy of ensemble with size of #100:\n",
      " [0.9029126213592233, 0.9514563106796117, 0.9611650485436893, 0.912621359223301, 0.9901960784313726] ---> AVG=0.9436702836474395\n"
     ]
    }
   ],
   "source": [
    "smoteboost_accuracies = []  # accuracies of different ensembles given 5 folds\n",
    "for es in ensemble_sizes:\n",
    "    kf_acc = []  # accuracies of 5 fold\n",
    "    for train_index, test_index in kfold.split(x):\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model = SMOTEBoost(x=x_train, y=y_train, n_classifier=es, \n",
    "                           smote_ratio=200, base=DecisionTreeClassifier, max_depth=1)\n",
    "        model.fit()\n",
    "        kf_acc.append(model.score(x_test, y_test))\n",
    "    smoteboost_accuracies.append(kf_acc)\n",
    "for idx,f in enumerate(smoteboost_accuracies):\n",
    "    print('Accuracy of ensemble with size of #{}:\\n {} ---> AVG={}'.format(\n",
    "        ensemble_sizes[idx], f, np.mean(smoteboost_accuracies[idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBBoost:\n",
    "    def __init__(self, x, y, n_classifier, k=5, base=None, weights=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize AdaBoost M2 (Weight init is same as M1)\n",
    "        \n",
    "        :param x: input feauture in shape of (samples, features)\n",
    "        :param y: input label in shape of (samples, )\n",
    "        :param base: base classifier (default Decision Tree)\n",
    "        :param n_classifier: number of base classifier in ensemble\n",
    "        :param weights: init model with pretrained weights\n",
    "        :param k: number of nearest neighbors in SMOTE\n",
    "        \n",
    "        :return: A RBBoost model\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.base = base\n",
    "        if self.base is None:\n",
    "            self.base = DecisionTreeClassifier\n",
    "        self.n_classifier = n_classifier\n",
    "        self.k = k\n",
    "        self.classifiers = []\n",
    "        self.weights = weights\n",
    "        self.alpha = []\n",
    "        self.newindex = 0  # to count SMOTEed samples\n",
    "        self.synthetic = []  # SMOTEed samples\n",
    "        self.bad_classifier_idx = []\n",
    "        \n",
    "        # init ensemble\n",
    "        for n in range(self.n_classifier):\n",
    "            self.classifiers.append(self.base(**kwargs))\n",
    "            \n",
    "        if self.weights is None:\n",
    "            # init weights using uniform distrobution\n",
    "            self.weights = np.ones(len(self.x)) / len(self.x)\n",
    "    \n",
    "    def __random_balance(self):\n",
    "        \"\"\"\n",
    "        Applies random balance algorithm to generate new data\n",
    "        \n",
    "        :return: a tuple of 2 numpy array of x and y (new_x, new_y, smoteing size)\n",
    "        \"\"\"\n",
    "        new_x = []\n",
    "        new_y = []\n",
    "        \n",
    "        total_size = len(self.x)\n",
    "        pos_size = len((self.y==1).nonzero()[0])\n",
    "        neg_size = len((self.y==0).nonzero()[0])\n",
    "        pos_data = self.x[self.y==1]\n",
    "        neg_data = self.x[self.y==0]\n",
    "        \n",
    "        if pos_size > neg_size:\n",
    "            self.major_data = pos_data\n",
    "            self.minor_data = neg_data\n",
    "            self.minor = 0\n",
    "        else:\n",
    "            self.minor_data = pos_data\n",
    "            self.major_data = neg_data\n",
    "            self.minor = 1\n",
    "            \n",
    "        majority_size = len(self.major_data)\n",
    "        minority_size = len(self.minor_data)\n",
    "        new_majority_size = np.random.randint(2, total_size - 2)\n",
    "        new_minority_size = total_size - new_majority_size\n",
    "        \n",
    "        if new_majority_size < majority_size:\n",
    "            new_x.extend(self.minor_data)\n",
    "            new_y.extend([1] * minority_size)\n",
    "            random_majority = random.sample(list(self.major_data), new_majority_size)\n",
    "            new_x.extend(random_majority)\n",
    "            new_y.extend([0] * new_majority_size)\n",
    "            smote = self.__SMOTE((new_minority_size-minority_size) * 100 / minority_size, self.minor_data)\n",
    "            new_x.extend(smote)\n",
    "            new_y.extend([1] * len(smote))\n",
    "        else:\n",
    "            new_x.extend(self.major_data)\n",
    "            new_y.extend([0] * majority_size)\n",
    "            random_minority = random.sample(list(self.minor_data), new_minority_size)\n",
    "            new_x.extend(random_minority)\n",
    "            new_y.extend([1] * new_minority_size)\n",
    "            smote = self.__SMOTE((new_majority_size-majority_size) * 100 / majority_size, self.major_data)\n",
    "            new_x.extend(smote)\n",
    "            new_y.extend([0] * len(smote))\n",
    "        return (np.array(new_x), np.array(new_y), len(smote))\n",
    "            \n",
    "    \n",
    "    def __SMOTE(self, smote_ratio, data):\n",
    "        \"\"\"\n",
    "        Applies SMOTE on data\n",
    "        \n",
    "        :param data: data to SMOTE\n",
    "        :param smote_ratio: The amount of SMOTEing data\n",
    "        :return: SMOTEed data in shape of (N*T/100)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.synthetic = []  # reinit synthetic for new SMOTEing\n",
    "        N = smote_ratio\n",
    "        T = N * len(data) / 100\n",
    "        if T % 2 != 0:  # just solving size mismatch\n",
    "            T = round(N * len(data) / 100)\n",
    "             \n",
    "        while T != 0:\n",
    "            i = np.random.randint(1, len(data)) - 1\n",
    "            self.__populate(i, self.__KNN(i, data), data)\n",
    "            T = T - 1\n",
    "        \n",
    "        return np.array(self.synthetic)\n",
    "        \n",
    "    def __KNN(self, idx, data):\n",
    "        \"\"\"\n",
    "        Applies SMOTE on data\n",
    "        \n",
    "        :param data: data to extract nearest neighbors\n",
    "        :param idx: index of an instance of input data (x)\n",
    "        :return: k indices of nearest neighbor to the given instance\n",
    "        \"\"\"\n",
    "        \n",
    "        distances = []\n",
    "        for i in range(len(data)):\n",
    "            if i != idx:\n",
    "                distances.append(((np.sqrt(np.sum(data[idx] - data[i])**2)), i))\n",
    "        # get k nearest\n",
    "        distances = sorted(distances, key=lambda x:x[0])\n",
    "        return [en[1] for en in distances[:self.k]]\n",
    "                \n",
    "    def __populate(self, i, knn, data):\n",
    "        \"\"\"\n",
    "        Create synthetic instances given particular instance and its K nearest neighbors\n",
    "        \n",
    "        :param data: data to generate artificial samples\n",
    "        :param i: index of current sample to generated SMOTE from\n",
    "        :param knn: index of k nearest neighbors of current sample i\n",
    "        :return: None - Updates self.synthetic \n",
    "        \"\"\"\n",
    "        \n",
    "        nn = np.random.randint(0, len(knn))\n",
    "        diff = data[knn[nn]] - data[i]\n",
    "        gap = np.random.randn(data.shape[1])\n",
    "        self.synthetic.insert(self.newindex, data[i] + gap * diff)\n",
    "        \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Train the ensemble using base weak classifiers\n",
    "        \"\"\"\n",
    "        for t in range(self.n_classifier):            \n",
    "            # SMOTE data\n",
    "            x_rb, y_rb, syn_size = self.__random_balance()\n",
    "            \n",
    "            # init artificial sample weights\n",
    "            w =  np.ones(len(self.x)) / len(self.x)\n",
    "            w[:-syn_size] = self.weights[:-syn_size]\n",
    "            self.weights = w\n",
    "            \n",
    "            # training weak classifier\n",
    "            self.classifiers[t].fit(x_rb, y_rb, self.weights)\n",
    "            \n",
    "            # calculating loss = sum of missclassified weights\n",
    "            miss_w = self.weights[(self.classifiers[t].predict(x_rb) != y_rb).nonzero()[0]]\n",
    "            loss = np.sum(miss_w) / 2 \n",
    "            \n",
    "            # calculating beta\n",
    "            a = loss / (1 - loss)\n",
    "            self.alpha.append(a)\n",
    "            \n",
    "            # drop bad classifiers\n",
    "            if self.classifiers[t].score(self.x, self.y) <= 0.5:\n",
    "                self.bad_classifier_idx.append(t)\n",
    "                continue\n",
    "            \n",
    "            # update weights\n",
    "            correct_pred_idx = (self.classifiers[t].predict(x_rb) == y_rb).nonzero()[0]\n",
    "            self.weights[correct_pred_idx] = self.weights[correct_pred_idx] * a\n",
    "            \n",
    "            # normalize weights\n",
    "            z = np.sum(self.weights)\n",
    "            self.weights = np.array([w / z for w in self.weights])\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict the class of given instance\n",
    "        \n",
    "        :param x: input feauture in shape of (samples, features)\n",
    "        \n",
    "        :return: a prediction of classes in label encoded form with shape of (samples, )\n",
    "        \"\"\"\n",
    "        \n",
    "        prediction = np.zeros((len(x),))\n",
    "        for idx in range(len(x)):\n",
    "            prediction[idx] = self.__predict_single_instance(x[idx].reshape(1, -1))\n",
    "        return prediction\n",
    "            \n",
    "    def __predict_single_instance(self, x):\n",
    "        \"\"\"\n",
    "        Predict the class of given instance\n",
    "        \n",
    "        :param x: input feauture in shape of (1, features)\n",
    "        \n",
    "        :return: a prediction of classes in label encoded form with shape of (1, )\n",
    "        \"\"\"\n",
    "        p = np.zeros((1, 2))\n",
    "        for n in range(self.n_classifier):\n",
    "            if n not in self.bad_classifier_idx:\n",
    "                if self.classifiers[n].predict(x) == 1:\n",
    "                    p[0,1] += np.log(1 / (self.alpha[n]+1e-10))\n",
    "                else:\n",
    "                    p[0,0] += np.log(1 / (self.alpha[n]+1e-10))\n",
    "        p[:,1] += 1e-10\n",
    "        return np.argmax(p, axis=1)\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        \"\"\"\n",
    "        Reports the score of model given x and y test\n",
    "        \"\"\"\n",
    "        p = self.predict(x)\n",
    "        return (p == y).nonzero()[0].__len__() / len(y)\n",
    "\n",
    "# test\n",
    "# model = RBBoost(x=x_train, y=y_train, n_classifier=30, base=DecisionTreeClassifier, max_depth=1)\n",
    "# model.fit()\n",
    "# model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of ensemble with size of #10:\n",
      " [0.912621359223301, 0.941747572815534, 0.970873786407767, 0.912621359223301, 0.9117647058823529] ---> AVG=0.9299257567104512\n",
      "Accuracy of ensemble with size of #50:\n",
      " [0.9223300970873787, 0.9514563106796117, 0.970873786407767, 0.941747572815534, 0.9901960784313726] ---> AVG=0.9553207690843328\n",
      "Accuracy of ensemble with size of #100:\n",
      " [0.912621359223301, 0.9514563106796117, 0.970873786407767, 0.9611650485436893, 0.9901960784313726] ---> AVG=0.9572625166571485\n"
     ]
    }
   ],
   "source": [
    "rbboost_accuracies = []  # accuracies of different ensembles given 5 folds\n",
    "for es in ensemble_sizes:\n",
    "    kf_acc = []  # accuracies of 5 fold\n",
    "    for train_index, test_index in kfold.split(x):\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model = RBBoost(x=x_train, y=y_train, n_classifier=es, base=DecisionTreeClassifier, max_depth=1)\n",
    "        model.fit()\n",
    "        kf_acc.append(model.score(x_test, y_test))\n",
    "    rbboost_accuracies.append(kf_acc)\n",
    "for idx,f in enumerate(rbboost_accuracies):\n",
    "    print('Accuracy of ensemble with size of #{}:\\n {} ---> AVG={}'.format(\n",
    "        ensemble_sizes[idx], f, np.mean(rbboost_accuracies[idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
