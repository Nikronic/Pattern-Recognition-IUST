{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Mcg   Gvh   Alm   Mit  Erl  Pox   Vac   Nuc     class\n",
      "0  0.51  0.40  0.56  0.17  0.5  0.5  0.49  0.22  negative\n",
      "1  0.40  0.39  0.60  0.15  0.5  0.0  0.58  0.30  negative\n",
      "2  0.40  0.42  0.57  0.35  0.5  0.0  0.53  0.25  negative\n",
      "3  0.46  0.44  0.52  0.11  0.5  0.0  0.50  0.22  negative\n",
      "4  0.47  0.39  0.50  0.11  0.5  0.0  0.49  0.40  negative\n",
      "Dataset X shape: (514, 8)\n",
      "Dataset Y shape: (514,)\n",
      "x_train shape: (462, 8)\n",
      "y_train shape: (462,)\n",
      "x_test shape: (52, 8)\n",
      "y_test shape: (52,)\n"
     ]
    }
   ],
   "source": [
    "header = ['Mcg', 'Gvh', 'Alm', 'Mit', 'Erl', 'Pox', 'Vac', 'Nuc', 'class']\n",
    "df = pd.read_csv('data/yeast-2_vs_4.dat', names=header, skiprows=13)\n",
    "print(df.head())\n",
    "df['class'] = df['class'].apply(lambda x:0 if x=='negative' else 1)\n",
    "\n",
    "df_np = df.to_numpy()\n",
    "x = df_np[:,:-1]\n",
    "y = df_np[:,-1]\n",
    "\n",
    "x = (x - x.mean(axis=0)) / x.var(axis=0)\n",
    "\n",
    "print('Dataset X shape:', x.shape)\n",
    "print('Dataset Y shape:', y.shape)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RUSBoost:\n",
    "    def __init__(self, x, y, n_classifier, base=None, weights=None):\n",
    "        \"\"\"\n",
    "        Initialize RUSBoost\n",
    "        \n",
    "        :param x: input feauture in shape of (samples, features)\n",
    "        :param y: input label in shape of (samples, )\n",
    "        :param base: base classifier (default Decision Tree)\n",
    "        :param n_classifier: number of base classifier in ensemble\n",
    "        :param weights: init model with pretrained weights\n",
    "        \n",
    "        :return: A RUSBoost model\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.base = base\n",
    "        if self.base is None:\n",
    "            self.base = DecisionTreeClassifier()\n",
    "        self.n_classifier = n_classifier\n",
    "        self.classifiers = []\n",
    "        self.weights = weights\n",
    "        self.alpha = []\n",
    "        \n",
    "        # init ensemble\n",
    "        for n in range(self.n_classifier):\n",
    "            self.classifiers.append(self.base)\n",
    "        \n",
    "        if self.weights is None:\n",
    "            # init weights using uniform distrobution\n",
    "            self.weights = np.ones((len(self.x))) / len(self.x)\n",
    "            \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict the class of given instance\n",
    "        \n",
    "        :param x: input feauture in shape of (samples, features)\n",
    "        \n",
    "        :return: a prediction of classes in label encoded form with shape of (samples, )\n",
    "        \"\"\"\n",
    "        \n",
    "        prediction = np.zeros((len(x),))\n",
    "        for idx in range(len(x)):\n",
    "            prediction[idx] = self.__predict_single_instance(x[idx].reshape(1, -1))\n",
    "        return prediction\n",
    "            \n",
    "    def __predict_single_instance(self, x):\n",
    "        \"\"\"\n",
    "        Predict the class of given instance\n",
    "        \n",
    "        :param x: input feauture in shape of (1, features)\n",
    "        \n",
    "        :return: a prediction of classes in label encoded form with shape of (1, )\n",
    "        \"\"\"\n",
    "        p = np.zeros((1, 2))\n",
    "        for n in range(self.n_classifier):\n",
    "            if self.classifiers[n].predict(x) == 1:\n",
    "                p[0,1] += np.log(1 / (self.alpha[n]+1e-10))\n",
    "            else:\n",
    "                p[0,0] += np.log(1 / (self.alpha[n]+1e-10))\n",
    "        eq_idx = (p[:,0] == p[:,1]).nonzero()[0]\n",
    "        p[eq_idx,self.minor] += 0.1\n",
    "        return np.argmax(p, axis=1)\n",
    "    \n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Train the ensemble using RUS data balancing and base weak classifiers\n",
    "        \"\"\"\n",
    "        for t in range(self.n_classifier):            \n",
    "            # random under sampling\n",
    "            rus_idx = self.__undersample()\n",
    "\n",
    "            # training weak classifier\n",
    "            self.classifiers[t].fit(self.x[rus_idx], self.y[rus_idx], self.weights[rus_idx])\n",
    "            \n",
    "            # calculating loss = sum of missclassified weights\n",
    "            \n",
    "            miss_w = self.weights[(self.classifiers[t].predict(self.x) != self.y).nonzero()[0]]\n",
    "            loss = np.sum(miss_w) / 2 \n",
    "            \n",
    "            # calculating beta\n",
    "            self.alpha.append(loss / (1 - loss))\n",
    "            \n",
    "            # update weights\n",
    "            \n",
    "            correct_pred_idx = (self.classifiers[t].predict(self.x) == self.y).nonzero()[0]\n",
    "            self.weights = self.weights * self.alpha[t]\n",
    "            \n",
    "            # normalize weights\n",
    "            z = np.sum(self.weights)\n",
    "            self.weights = np.array([w / z for w in self.weights])\n",
    "             \n",
    "    \n",
    "    def score(self, x, y):\n",
    "        p = self.predict(x)\n",
    "        return (p == y).nonzero()[0].__len__() / len(y)\n",
    "           \n",
    "    def __undersample(self):\n",
    "        \"\"\"\n",
    "        Generates a random unique subset of majority data as same size as minority and return the indices\n",
    "        \n",
    "        :return: A sorted list of indices with shape of (2*minority_data, )\n",
    "        \"\"\"\n",
    "        pos_size = len((self.y==1).nonzero()[0])\n",
    "        neg_size = len((self.y==0).nonzero()[0])\n",
    "        pos_data = self.x[self.y==1]\n",
    "        neg_data = self.x[self.y==0]\n",
    "        \n",
    "        if pos_size > neg_size:\n",
    "            self.major_data = pos_data\n",
    "            self.minor_data = neg_data\n",
    "            self.minor = 0\n",
    "        else:\n",
    "            self.minor_data = pos_data\n",
    "            self.major_data = neg_data\n",
    "            self.minor = 1\n",
    "        # getting index of sampled intances for enabling correct weight update\n",
    "        minor_idx = (self.y == self.minor).nonzero()[0]\n",
    "        major_idx = (self.y == int(not self.minor)).nonzero()[0]\n",
    "        major_idx = np.array(random.sample(list(major_idx), len(self.minor_data)))\n",
    "        return sorted(np.concatenate((minor_idx, major_idx)))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9038461538461539"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RUSBoost(x=x_train, y=y_train, n_classifier=30, base=DecisionTreeClassifier(max_depth=1))\n",
    "model.fit()\n",
    "model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoostM2:\n",
    "    def __init__(self, x, y, n_classifier, base=None, weights=None):\n",
    "        \"\"\"\n",
    "        Initialize AdaBoost M2 (Weight init is same as M1)\n",
    "        \n",
    "        :param x: input feauture in shape of (samples, features)\n",
    "        :param y: input label in shape of (samples, )\n",
    "        :param base: base classifier (default Decision Tree)\n",
    "        :param n_classifier: number of base classifier in ensemble\n",
    "        :param weights: init model with pretrained weights\n",
    "        \n",
    "        :return: A AdaBoost model\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.base = base\n",
    "        if self.base is None:\n",
    "            self.base = DecisionTreeClassifier()\n",
    "        self.n_classifier = n_classifier\n",
    "        self.classifiers = []\n",
    "        self.weights = weights\n",
    "        self.alpha = []\n",
    "        \n",
    "        # init ensemble\n",
    "        for n in range(self.n_classifier):\n",
    "            self.classifiers.append(self.base)\n",
    "        \n",
    "        if self.weights is None:\n",
    "            # init weights using uniform distrobution\n",
    "            self.weights = np.ones((len(self.x))) / len(self.x)\n",
    "            \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict the class of given instance\n",
    "        \n",
    "        :param x: input feauture in shape of (samples, features)\n",
    "        \n",
    "        :return: a prediction of classes in label encoded form with shape of (samples, )\n",
    "        \"\"\"\n",
    "        \n",
    "        prediction = np.zeros((len(x),))\n",
    "        for idx in range(len(x)):\n",
    "            prediction[idx] = self.__predict_single_instance(x[idx].reshape(1, -1))\n",
    "        return prediction\n",
    "            \n",
    "    def __predict_single_instance(self, x):\n",
    "        \"\"\"\n",
    "        Predict the class of given instance\n",
    "        \n",
    "        :param x: input feauture in shape of (1, features)\n",
    "        \n",
    "        :return: a prediction of classes in label encoded form with shape of (1, )\n",
    "        \"\"\"\n",
    "        p = np.zeros((1, 2))\n",
    "        for n in range(self.n_classifier):\n",
    "            if self.classifiers[n].predict(x) == 1:\n",
    "                p[0,1] += np.log(1 / (self.alpha[n]+1e-10))\n",
    "            else:\n",
    "                p[0,0] += np.log(1 / (self.alpha[n]+1e-10))\n",
    "        return np.argmax(p, axis=1)\n",
    "    \n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Train the ensemble using base weak classifiers\n",
    "        \"\"\"\n",
    "        for t in range(self.n_classifier):            \n",
    "            \n",
    "            # training weak classifier\n",
    "            self.classifiers[t].fit(self.x, self.y, self.weights)\n",
    "            \n",
    "            # calculating loss = sum of missclassified weights\n",
    "            miss_w = self.weights[(self.classifiers[t].predict(self.x) != self.y).nonzero()[0]]\n",
    "            loss = np.sum(miss_w) / 2 \n",
    "            \n",
    "            # calculating beta\n",
    "            self.alpha.append(loss / (1 - loss))\n",
    "            \n",
    "            # update weights\n",
    "            \n",
    "            correct_pred_idx = (self.classifiers[t].predict(self.x) == self.y).nonzero()[0]\n",
    "            self.weights = self.weights * self.alpha[t]\n",
    "            \n",
    "            # normalize weights\n",
    "            z = np.sum(self.weights)\n",
    "            self.weights = np.array([w / z for w in self.weights])\n",
    "             \n",
    "    \n",
    "    def score(self, x, y):\n",
    "        p = self.predict(x)\n",
    "        return (p == y).nonzero()[0].__len__() / len(y)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9230769230769231"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AdaBoostM2(x=x_train, y=y_train, n_classifier=30, base=DecisionTreeClassifier(max_depth=1))\n",
    "model.fit()\n",
    "model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMOTEBoost:\n",
    "    def __init__(self, x, y, n_classifier, k=5, smote_ratio=100, base=None, weights=None):\n",
    "        \"\"\"\n",
    "        Initialize AdaBoost M2 (Weight init is same as M1)\n",
    "        \n",
    "        :param x: input feauture in shape of (samples, features)\n",
    "        :param y: input label in shape of (samples, )\n",
    "        :param base: base classifier (default Decision Tree)\n",
    "        :param n_classifier: number of base classifier in ensemble\n",
    "        :param weights: init model with pretrained weights\n",
    "        :param smote_ratio: the ratio of smoteing data\n",
    "        :param k: number of nearest neighbors in SMOTE\n",
    "        \n",
    "        :return: A SMOTEBoost model\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.base = base\n",
    "        if self.base is None:\n",
    "            self.base = DecisionTreeClassifier()\n",
    "        self.n_classifier = n_classifier\n",
    "        self.smote_ratio = smote_ratio  # alias N\n",
    "        self.k = k\n",
    "        self.classifiers = []\n",
    "        self.weights = weights\n",
    "        self.alpha = []\n",
    "        self.newindex = 0  # to count SMOTEed samples\n",
    "        self.synthetic = []  # SMOTEed samples\n",
    "        \n",
    "        # init ensemble\n",
    "        for n in range(self.n_classifier):\n",
    "            self.classifiers.append(self.base)\n",
    "            \n",
    "    def __SMOTE(self):\n",
    "        \"\"\"\n",
    "        Applies SMOTE on data\n",
    "        \n",
    "        :return: SMOTEed data in shape of (N*T/100)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.synthetic = []  # reinit synthetic for new SMOTEing\n",
    "        \n",
    "        pos_size = len((self.y==1).nonzero()[0])\n",
    "        neg_size = len((self.y==0).nonzero()[0])\n",
    "        pos_data = self.x[self.y==1]\n",
    "        neg_data = self.x[self.y==0]\n",
    "        \n",
    "        if pos_size > neg_size:\n",
    "            self.major_data = pos_data\n",
    "            self.minor_data = neg_data\n",
    "            self.minor = 0\n",
    "        else:\n",
    "            self.minor_data = pos_data\n",
    "            self.major_data = neg_data\n",
    "            self.minor = 1\n",
    "        \n",
    "        N = self.smote_ratio\n",
    "        T = len(self.minor_data)\n",
    "        T = int(N * T / 100)\n",
    "             \n",
    "        while T != 0:\n",
    "            i = np.random.randint(1, len(self.minor_data)) - 1\n",
    "            self.__populate(i, self.__KNN(i))\n",
    "            T = T - 1\n",
    "        \n",
    "        return np.array(self.synthetic)\n",
    "        \n",
    "    def __KNN(self, idx):\n",
    "        \"\"\"\n",
    "        Applies SMOTE on data\n",
    "        \n",
    "        :param idx: index of an instance of input data (x)\n",
    "        :return: k indices of nearest neighbor to the given instance\n",
    "        \"\"\"\n",
    "        \n",
    "        distances = []\n",
    "        for i in range(len(self.minor_data)):\n",
    "            if i != idx:\n",
    "                distances.append(((np.sqrt(np.sum(self.minor_data[idx] - self.minor_data[i])**2)), i))\n",
    "        # get k nearest\n",
    "        distances = sorted(distances, key=lambda x:x[0])\n",
    "        return [en[1] for en in distances[:self.k]]\n",
    "                \n",
    "    def __populate(self, i, knn):\n",
    "        \"\"\"\n",
    "        Create synthetic instances given particular instance and its K nearest neighbors\n",
    "        \n",
    "        :param i: index of current sample to generated SMOTE from\n",
    "        :param knn: index of k nearest neighbors of current sample i\n",
    "        :return: None - Updates self.synthetic \n",
    "        \"\"\"\n",
    "        \n",
    "        nn = np.random.randint(0, len(knn))\n",
    "        diff = self.minor_data[knn[nn]] - self.minor_data[i]\n",
    "        gap = np.random.randn(self.minor_data.shape[1])\n",
    "        self.synthetic.insert(self.newindex, self.minor_data[i] + gap * diff)\n",
    "        \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Train the ensemble using base weak classifiers\n",
    "        \"\"\"\n",
    "        syn_data = self.__SMOTE()  # just to determine sizes\n",
    "        if self.weights is None:\n",
    "            # init weights using uniform distrobution\n",
    "            self.weights = np.ones((len(syn_data)+len(self.x))) / (len(syn_data)+len(self.x))\n",
    "        \n",
    "        for t in range(self.n_classifier):            \n",
    "            # SMOTE data\n",
    "            syn_data = self.__SMOTE()\n",
    "            x_smote = np.concatenate((self.x, syn_data))\n",
    "            y_smote = np.concatenate((self.y, np.ones((len(syn_data)))*self.minor))\n",
    "            \n",
    "            # training weak classifier\n",
    "            self.classifiers[t].fit(x_smote, y_smote, self.weights)\n",
    "            \n",
    "            # calculating loss = sum of missclassified weights\n",
    "            miss_w = self.weights[(self.classifiers[t].predict(x_smote) != y_smote).nonzero()[0]]\n",
    "            loss = np.sum(miss_w) / 2 \n",
    "            \n",
    "            # calculating beta\n",
    "            self.alpha.append(loss / (1 - loss))\n",
    "            \n",
    "            # update weights\n",
    "            \n",
    "            correct_pred_idx = (self.classifiers[t].predict(x_smote) == y_smote).nonzero()[0]\n",
    "            self.weights = self.weights * self.alpha[t]\n",
    "            \n",
    "            # normalize weights\n",
    "            z = np.sum(self.weights)\n",
    "            self.weights = np.array([w / z for w in self.weights])\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict the class of given instance\n",
    "        \n",
    "        :param x: input feauture in shape of (samples, features)\n",
    "        \n",
    "        :return: a prediction of classes in label encoded form with shape of (samples, )\n",
    "        \"\"\"\n",
    "        \n",
    "        prediction = np.zeros((len(x),))\n",
    "        for idx in range(len(x)):\n",
    "            prediction[idx] = self.__predict_single_instance(x[idx].reshape(1, -1))\n",
    "        return prediction\n",
    "            \n",
    "    def __predict_single_instance(self, x):\n",
    "        \"\"\"\n",
    "        Predict the class of given instance\n",
    "        \n",
    "        :param x: input feauture in shape of (1, features)\n",
    "        \n",
    "        :return: a prediction of classes in label encoded form with shape of (1, )\n",
    "        \"\"\"\n",
    "        p = np.zeros((1, 2))\n",
    "        for n in range(self.n_classifier):\n",
    "            if self.classifiers[n].predict(x) == 1:\n",
    "                p[0,1] += np.log(1 / (self.alpha[n]+1e-10))\n",
    "            else:\n",
    "                p[0,0] += np.log(1 / (self.alpha[n]+1e-10))\n",
    "        return np.argmax(p, axis=1)\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        p = self.predict(x)\n",
    "        return (p == y).nonzero()[0].__len__() / len(y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9230769230769231"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SMOTEBoost(x=x_train, y=y_train, n_classifier=50, smote_ratio=200, base=DecisionTreeClassifier(max_depth=1))\n",
    "model.fit()\n",
    "model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
