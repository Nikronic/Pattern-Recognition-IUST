{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Mcg   Gvh   Alm   Mit  Erl  Pox   Vac   Nuc     class\n",
      "0  0.51  0.40  0.56  0.17  0.5  0.5  0.49  0.22  negative\n",
      "1  0.40  0.39  0.60  0.15  0.5  0.0  0.58  0.30  negative\n",
      "2  0.40  0.42  0.57  0.35  0.5  0.0  0.53  0.25  negative\n",
      "3  0.46  0.44  0.52  0.11  0.5  0.0  0.50  0.22  negative\n",
      "4  0.47  0.39  0.50  0.11  0.5  0.0  0.49  0.40  negative\n",
      "Dataset X shape: (514, 8)\n",
      "Dataset Y shape: (514,)\n",
      "x_train shape: (462, 8)\n",
      "y_train shape: (462,)\n",
      "x_test shape: (52, 8)\n",
      "y_test shape: (52,)\n"
     ]
    }
   ],
   "source": [
    "header = ['Mcg', 'Gvh', 'Alm', 'Mit', 'Erl', 'Pox', 'Vac', 'Nuc', 'class']\n",
    "df = pd.read_csv('data/yeast-2_vs_4.dat', names=header, skiprows=13)\n",
    "print(df.head())\n",
    "df['class'] = df['class'].apply(lambda x:0 if x=='negative' else 1)\n",
    "\n",
    "df_np = df.to_numpy()\n",
    "x = df_np[:,:-1]\n",
    "y = df_np[:,-1]\n",
    "\n",
    "print('Dataset X shape:', x.shape)\n",
    "print('Dataset Y shape:', y.shape)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RUSBoost:\n",
    "    def __init__(self, x, y, n_classifier, base=None, weights=None):\n",
    "        \"\"\"\n",
    "        Initialize RUSBoost\n",
    "        \n",
    "        :param x: input feauture in shape of (samples, features)\n",
    "        :param y: input label in shape of (samples, )\n",
    "        :param base: base classifier (default Decision Tree)\n",
    "        :param n_classifier: number of base classifier in ensemble\n",
    "        :param weights: init model with pretrained weights\n",
    "        \n",
    "        :return: A RUSBoost model\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.base = base\n",
    "        if self.base is None:\n",
    "            self.base = DecisionTreeClassifier()\n",
    "        self.n_classifier = n_classifier\n",
    "        self.classifiers = []\n",
    "        self.weights = weights\n",
    "        self.alpha = []\n",
    "        \n",
    "        # init ensemble\n",
    "        for n in range(self.n_classifier):\n",
    "            self.classifiers.append(self.base)\n",
    "        \n",
    "        if self.weights is None:\n",
    "            # init weights using uniform distrobution\n",
    "            self.weights = np.ones((len(self.x))) / len(self.x)\n",
    "            \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict the class of given instance\n",
    "        \n",
    "        :param x: input feauture in shape of (samples, features)\n",
    "        \n",
    "        :return: a prediction of classes in label encoded form with shape of (samples, )\n",
    "        \"\"\"\n",
    "        \n",
    "        prediction = np.zeros((len(x),))\n",
    "        for idx in range(len(x)):\n",
    "            prediction[idx] = self.__predict_single_instance(x[idx].reshape(1, -1))\n",
    "        return prediction\n",
    "            \n",
    "    def __predict_single_instance(self, x):\n",
    "        \"\"\"\n",
    "        Predict the class of given instance\n",
    "        \n",
    "        :param x: input feauture in shape of (1, features)\n",
    "        \n",
    "        :return: a prediction of classes in label encoded form with shape of (1, )\n",
    "        \"\"\"\n",
    "        p = np.zeros((1, 2))\n",
    "        for n in range(self.n_classifier):\n",
    "            if self.classifiers[n].predict(x) == 1:\n",
    "                p[0,1] += np.log(1 / self.alpha[n])\n",
    "            else:\n",
    "                p[0,0] += np.log(1 / self.alpha[n])\n",
    "        eq_idx = (p[:,0] == p[:,1]).nonzero()[0]\n",
    "        p[eq_idx,self.minor] += 0.1\n",
    "        return np.argmax(p, axis=1)\n",
    "    \n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Train the ensemble using RUS data balancing and base weak classifiers\n",
    "        \"\"\"\n",
    "        for t in range(self.n_classifier):            \n",
    "            # random under sampling\n",
    "            rus_idx = self.__undersample()\n",
    "\n",
    "            # training weak classifier\n",
    "            self.classifiers[t].fit(self.x[rus_idx], self.y[rus_idx], self.weights[rus_idx])\n",
    "            \n",
    "            # calculating loss = sum of missclassified weights\n",
    "            \n",
    "            miss_w = self.weights[(self.classifiers[t].predict(self.x) != self.y).nonzero()[0]]\n",
    "            loss = np.sum(miss_w) / 2 \n",
    "            \n",
    "            # calculating beta\n",
    "            self.alpha.append(loss / (1 - loss))\n",
    "            \n",
    "            # update weights\n",
    "            \n",
    "            correct_pred_idx = (self.classifiers[t].predict(self.x) == self.y).nonzero()[0]\n",
    "            self.weights = self.weights * self.alpha[t]\n",
    "            \n",
    "            # normalize weights\n",
    "            z = np.sum(self.weights)\n",
    "            self.weights = np.array([w / z for w in self.weights])\n",
    "             \n",
    "    \n",
    "    def score(self, x, y):\n",
    "        p = self.predict(x)\n",
    "        return (p == y).nonzero()[0].__len__() / len(y)\n",
    "           \n",
    "    def __undersample(self):\n",
    "        \"\"\"\n",
    "        Generates a random unique subset of majority data as same size as minority and return the indices\n",
    "        \n",
    "        :return: A sorted list of indices with shape of (2*minority_data, )\n",
    "        \"\"\"\n",
    "        pos_size = len((self.y==1).nonzero()[0])\n",
    "        neg_size = len((self.y==0).nonzero()[0])\n",
    "        pos_data = self.x[self.y==1]\n",
    "        neg_data = self.x[self.y==0]\n",
    "        \n",
    "        if pos_size > neg_size:\n",
    "            self.major_data = pos_data\n",
    "            self.minor_data = neg_data\n",
    "            self.minor = 0\n",
    "        else:\n",
    "            self.minor_data = pos_data\n",
    "            self.major_data = neg_data\n",
    "            self.minor = 1\n",
    "        # getting index of sampled intances for enabling correct weight update\n",
    "        minor_idx = (self.y == self.minor).nonzero()[0]\n",
    "        major_idx = (self.y == int(not self.minor)).nonzero()[0]\n",
    "        major_idx = np.array(random.sample(list(major_idx), len(self.minor_data)))\n",
    "        return sorted(np.concatenate((minor_idx, major_idx)))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9423076923076923"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RUSBoost(x=x_train, y=y_train, n_classifier=400, base=SVC(gamma='scale'))\n",
    "model.fit()\n",
    "model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84615384615384615"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "m = AdaBoostClassifier(base_estimator=SVC(gamma='scale'),n_estimators=30, algorithm='SAMME')\n",
    "m = m.fit(x_train, y_train)\n",
    "m.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92307692307692313"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "m = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=30, algorithm='SAMME')\n",
    "m = m.fit(x_train, y_train)\n",
    "m.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoostM2:\n",
    "    def __init__(self, x, y, n_classifier, base=None, weights=None):\n",
    "        \"\"\"\n",
    "        Initialize AdaBoost M2 (Weight init is same as M1)\n",
    "        \n",
    "        :param x: input feauture in shape of (samples, features)\n",
    "        :param y: input label in shape of (samples, )\n",
    "        :param base: base classifier (default Decision Tree)\n",
    "        :param n_classifier: number of base classifier in ensemble\n",
    "        :param weights: init model with pretrained weights\n",
    "        \n",
    "        :return: A AdaBoost model\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.base = base\n",
    "        if self.base is None:\n",
    "            self.base = DecisionTreeClassifier()\n",
    "        self.n_classifier = n_classifier\n",
    "        self.classifiers = []\n",
    "        self.weights = weights\n",
    "        self.alpha = []\n",
    "        \n",
    "        # init ensemble\n",
    "        for n in range(self.n_classifier):\n",
    "            self.classifiers.append(self.base)\n",
    "        \n",
    "        if self.weights is None:\n",
    "            # init weights using uniform distrobution\n",
    "            self.weights = np.ones((len(self.x))) / len(self.x)\n",
    "            \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict the class of given instance\n",
    "        \n",
    "        :param x: input feauture in shape of (samples, features)\n",
    "        \n",
    "        :return: a prediction of classes in label encoded form with shape of (samples, )\n",
    "        \"\"\"\n",
    "        \n",
    "        prediction = np.zeros((len(x),))\n",
    "        for idx in range(len(x)):\n",
    "            prediction[idx] = self.__predict_single_instance(x[idx].reshape(1, -1))\n",
    "        return prediction\n",
    "            \n",
    "    def __predict_single_instance(self, x):\n",
    "        \"\"\"\n",
    "        Predict the class of given instance\n",
    "        \n",
    "        :param x: input feauture in shape of (1, features)\n",
    "        \n",
    "        :return: a prediction of classes in label encoded form with shape of (1, )\n",
    "        \"\"\"\n",
    "        p = np.zeros((1, 2))\n",
    "        for n in range(self.n_classifier):\n",
    "            if self.classifiers[n].predict(x) == 1:\n",
    "                p[0,1] += np.log(1 / self.alpha[n])\n",
    "            else:\n",
    "                p[0,0] += np.log(1 / self.alpha[n])\n",
    "        return np.argmax(p, axis=1)\n",
    "    \n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Train the ensemble using base weak classifiers\n",
    "        \"\"\"\n",
    "        for t in range(self.n_classifier):            \n",
    "            \n",
    "            # training weak classifier\n",
    "            self.classifiers[t].fit(self.x, self.y, self.weights)\n",
    "            \n",
    "            # calculating loss = sum of missclassified weights\n",
    "            miss_w = self.weights[(self.classifiers[t].predict(self.x) != self.y).nonzero()[0]]\n",
    "            loss = np.sum(miss_w) / 2 \n",
    "            \n",
    "            # calculating beta\n",
    "            self.alpha.append(loss / (1 - loss))\n",
    "            \n",
    "            # update weights\n",
    "            \n",
    "            correct_pred_idx = (self.classifiers[t].predict(self.x) == self.y).nonzero()[0]\n",
    "            self.weights = self.weights * self.alpha[t]\n",
    "            \n",
    "            # normalize weights\n",
    "            z = np.sum(self.weights)\n",
    "            self.weights = np.array([w / z for w in self.weights])\n",
    "             \n",
    "    \n",
    "    def score(self, x, y):\n",
    "        p = self.predict(x)\n",
    "        return (p == y).nonzero()[0].__len__() / len(y)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9038461538461539"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AdaBoostM2(x=x_train, y=y_train, n_classifier=30, base=SVC(gamma='scale'))\n",
    "model.fit()\n",
    "model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
